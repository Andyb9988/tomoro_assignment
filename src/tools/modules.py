from logging import Logger
from typing import (
    Any,
    List,
)

import dspy

from src.tools.signatures import (
    AssessReasoning,
    GenerateCotAnswer,
)
from src.utils.logging_utils import get_logger

logger: Logger = get_logger(name=__name__)


class OutputFinalAnswer(dspy.Module):
    """
    A module to generate and retrieve a final answer to a single question
    based on provided context using Chain of Thought reasoning.

    Attributes:
        context (str): The context within which the question is answered.
        question (str): The question to answer.
        cot_answer (dspy.ChainOfThought): Chain of Thought instance for generating the answer.
    """

    def __init__(self, context: str, id: str) -> None:
        """
        Initializes OutputFinalAnswerSingle with context and question.

        Args:
            context (str): The context within which the question is answered.
            question (str): The question to be answered.
        """
        super().__init__()
        self.generate_answer = dspy.ChainOfThought(GenerateCotAnswer)
        self.context = context
        self.id = id

    def get_answer(self, question: str) -> Any:
        """
        Generates an answer using Chain of Thought reasoning based on context and question.

        Returns:
            Any: The final answer generated by the Chain of Thought model.
        """
        logger.info(
            f"Generating answer for the single {question} using provided context."
        )

        prediction = self.generate_answer(question=question, context=self.context)
        return prediction


class LLMJudge(dspy.Module):
    def __init__(
        self,
        context: str,
        llm_reasoning: str,
        dialogue_break: str,
    ) -> None:
        super().__init__()
        self.generate_answer = dspy.Predict(AssessReasoning)
        self.context = context
        self.llm_reasoning = llm_reasoning
        self.dialogue_break = dialogue_break

    def get_answer(self) -> Any:
        prediction = self.generate_answer(
            actual_reasoning=self.dialogue_break,
            llm_reasoning=self.llm_reasoning,
            context=self.context,
        )
        return prediction
